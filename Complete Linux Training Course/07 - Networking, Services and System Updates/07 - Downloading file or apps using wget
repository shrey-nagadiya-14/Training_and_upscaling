# wget Command - Complete Guide

## Table of Contents
1. [What is wget?](#what-is-wget)
2. [wget vs curl](#wget-vs-curl)
3. [Basic Syntax & Options](#basic-syntax--options)
4. [wget Examples](#wget-examples)
5. [Daily Use Cases](#daily-use-cases)
6. [Real-Life Scenarios](#real-life-scenarios)
7. [Advanced Techniques](#advanced-techniques)
8. [Best Practices](#best-practices)
9. [Common Errors & Solutions](#common-errors--solutions)
10. [Quick Reference](#quick-reference)

---

## 1. What is wget?

### Definition
- **wget** stands for "Web Get"
- A free utility for non-interactive download of files from the web
- Supports HTTP, HTTPS, and FTP protocols
- Works in the background, even without user interaction
- Perfect for downloading files on servers without GUI
- Can resume interrupted downloads automatically

### Key Characteristics
✅ Non-interactive (can run in background)
✅ Recursive downloads (download entire websites)
✅ Automatic retry on connection failure
✅ Supports bandwidth throttling
✅ Can download multiple files
✅ Works well in scripts and cron jobs
✅ Handles slow/unstable connections better than browsers

### Why wget in Corporate Environments?

Problem: Most corporate servers do NOT have internet access directly
Solution: wget allows you to download files on your local machine, then transfer to server

Typical Corporate Workflow:
┌──────────────────┐         ┌──────────────────┐         ┌──────────────────┐
│   Your Laptop    │         │   Jump Server    │         │ Production Server│
│  (Has Internet)  │────────▶│ (Limited Access) │────────▶│  (No Internet)   │
└──────────────────┘         └──────────────────┘         └──────────────────┘
      wget here                 Transfer here              Use file here
   Downloads file                    SCP/SFTP              Application runs

Real-World Example:
# On your laptop (has internet):
wget https://downloads.mysql.com/archives/get/p/23/file/mysql-8.0.35.tar.gz

# Transfer to server (no internet):
scp mysql-8.0.35.tar.gz username@production-server:/tmp/

# On production server (no internet):
cd /tmp
tar -xzf mysql-8.0.35.tar.gz
# Install MySQL

---

## 2. wget vs curl

### Comparison Table

| Feature | wget | curl |
|---------|------|------|
| Purpose | Downloading files | Data transfer & testing |
| Recursive download | ✅ Yes | ❌ No |
| Resume downloads | ✅ Automatic | ✅ Manual (-C -) |
| Multiple files | ✅ Easy | ⚠️ Requires loops |
| API testing | ❌ Limited | ✅ Excellent |
| Mirror websites | ✅ Yes | ❌ No |
| Background operation | ✅ Native | ⚠️ With nohup |
| Protocols | HTTP, HTTPS, FTP | 25+ protocols |
| Upload files | ❌ No | ✅ Yes |
| POST requests | ⚠️ Limited | ✅ Full support |
| Cookie handling | ✅ Good | ✅ Excellent |
| Proxy support | ✅ Yes | ✅ Yes |
| Script friendly | ✅ Very | ✅ Very |

### When to Use wget
✅ Downloading large files
✅ Downloading multiple files
✅ Mirroring websites
✅ Recursive downloads
✅ Long-running downloads
✅ Automated backup downloads
✅ Downloading in background

### When to Use curl
✅ API testing
✅ Viewing web page content
✅ Uploading files
✅ Complex HTTP operations
✅ Quick header checks
✅ Testing webhooks
✅ Authentication testing

---

## 3. Basic Syntax & Options

### Basic Syntax
wget [options] [URL]

### Essential wget Options

| Option | Description | Example |
|--------|-------------|---------|
| `-O` | Save with custom filename | wget -O myfile.zip http://example.com/file.zip |
| `-P` | Save to specific directory | wget -P /downloads/ http://example.com/file.zip |
| `-c` | Continue/resume partial download | wget -c http://example.com/largefile.iso |
| `-b` | Run in background | wget -b http://example.com/file.zip |
| `-i` | Download URLs from file | wget -i urls.txt |
| `-r` | Recursive download | wget -r http://example.com |
| `-l` | Maximum recursion depth | wget -r -l 2 http://example.com |
| `-np` | No parent (don't ascend) | wget -r -np http://example.com/docs/ |
| `-k` | Convert links for offline viewing | wget -r -k http://example.com |
| `-m` | Mirror (shortcut for -r -N -l inf) | wget -m http://example.com |
| `-q` | Quiet mode (no output) | wget -q http://example.com/file.zip |
| `-v` | Verbose mode | wget -v http://example.com/file.zip |
| `--limit-rate` | Limit download speed | wget --limit-rate=200k http://example.com/file.zip |
| `--user` | HTTP username | wget --user=admin http://example.com/file.zip |
| `--password` | HTTP password | wget --password=secret http://example.com/file.zip |
| `--no-check-certificate` | Ignore SSL errors | wget --no-check-certificate https://example.com/file.zip |
| `-U` | Set User-Agent | wget -U "Mozilla/5.0" http://example.com |
| `--spider` | Check if URL exists | wget --spider http://example.com/file.zip |
| `-t` | Number of retries | wget -t 5 http://example.com/file.zip |
| `-T` | Timeout in seconds | wget -T 30 http://example.com/file.zip |

---

## 4. wget Examples

### Example 1: Simple File Download
# Download a file (saves with original name)
wget https://wordpress.org/latest.zip

Output:
--2025-11-21 10:30:00--  https://wordpress.org/latest.zip
Resolving wordpress.org (wordpress.org)... 198.143.164.252
Connecting to wordpress.org (wordpress.org)|198.143.164.252|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 24575123 (23M) [application/zip]
Saving to: 'latest.zip'

latest.zip          100%[===================>]  23.43M  5.20MB/s    in 5.2s

2025-11-21 10:30:05 (4.50 MB/s) - 'latest.zip' saved [24575123/24575123]

### Example 2: Download with Custom Filename
# Save file with different name
wget -O wordpress.zip https://wordpress.org/latest.zip

# File saved as: wordpress.zip

### Example 3: Download to Specific Directory
# Save to specific location
wget -P /home/user/downloads/ https://wordpress.org/latest.zip

# File saved to: /home/user/downloads/latest.zip

### Example 4: Resume Interrupted Download
# Start download
wget https://releases.ubuntu.com/22.04/ubuntu-22.04-desktop-amd64.iso

# If interrupted (Ctrl+C or connection lost), resume with:
wget -c https://releases.ubuntu.com/22.04/ubuntu-22.04-desktop-amd64.iso

# Downloads only the remaining part, not the entire file again!

### Example 5: Background Download
# Download in background
wget -b https://example.com/large-file.zip

Output:
Continuing in background, pid 12345.
Output will be written to 'wget-log'.

# Check progress:
tail -f wget-log

# Check if still running:
ps aux | grep wget

### Example 6: Limit Download Speed
# Limit to 500 KB/s (useful to avoid network congestion)
wget --limit-rate=500k https://example.com/large-file.zip

# Limit to 1 MB/s:
wget --limit-rate=1m https://example.com/large-file.zip

### Example 7: Download Multiple Files from List
# Create file with URLs (urls.txt):
https://example.com/file1.zip
https://example.com/file2.tar.gz
https://example.com/file3.pdf

# Download all:
wget -i urls.txt

# Download in background with custom log:
wget -b -i urls.txt -o download.log

### Example 8: Download with Authentication
# Basic authentication
wget --user=admin --password=secret123 https://secure.example.com/file.zip

# Or prompt for password:
wget --user=admin --ask-password https://secure.example.com/file.zip

# Using FTP with credentials:
wget ftp://username:password@ftp.example.com/file.zip

### Example 9: Mirror/Clone Website
# Download entire website for offline viewing
wget --mirror --convert-links --adjust-extension --page-requisites --no-parent \
  http://example.com

Breakdown:
--mirror (-m): Turns on recursion and timestamping
--convert-links (-k): Convert links for offline viewing
--adjust-extension (-E): Add .html extension to files
--page-requisites (-p): Download CSS, images, etc.
--no-parent (-np): Don't ascend to parent directory

### Example 10: Download Specific File Types
# Download all PDFs from a website
wget -r -l1 -A.pdf http://example.com/documents/

Options:
-r: Recursive
-l1: Depth 1 (only this directory level)
-A.pdf: Accept only PDF files

# Download all images (jpg, png, gif):
wget -r -l1 -A.jpg,.png,.gif http://example.com/gallery/

### Example 11: Reject Specific File Types
# Download everything except videos
wget -r -R "*.mp4,*.avi,*.mov" http://example.com/

Options:
-R: Reject these file types

### Example 12: Check If URL Exists (Spider Mode)
# Test if file exists without downloading
wget --spider https://example.com/file.zip

Output if exists:
Spider mode enabled. Check if remote file exists.
HTTP request sent, awaiting response... 200 OK
Length: 1234567 (1.2M) [application/zip]
Remote file exists.

Output if doesn't exist:
HTTP request sent, awaiting response... 404 Not Found
Remote file does not exist -- broken link!!!

### Example 13: Download with Retry Options
# Retry 10 times with 30-second timeout
wget -t 10 -T 30 https://unreliable-server.com/file.zip

# Infinite retries (useful for unstable connections):
wget -t 0 https://unreliable-server.com/file.zip

### Example 14: Quiet Mode (No Output)
# Silent download (useful in scripts)
wget -q https://example.com/file.zip

# Check if download succeeded:
if wget -q https://example.com/file.zip; then
  echo "Download successful"
else
  echo "Download failed"
fi

### Example 15: FTP Download
# Anonymous FTP:
wget ftp://ftp.example.com/pub/file.zip

# FTP with credentials:
wget ftp://username:password@ftp.example.com/private/file.zip

# Download entire FTP directory:
wget -r ftp://username:password@ftp.example.com/directory/

### Example 16: Download with Custom Headers
# Add custom headers
wget --header="Authorization: Bearer TOKEN123" \
     --header="X-Custom-Header: value" \
     https://api.example.com/download/file.zip

### Example 17: Ignore SSL Certificate Errors
# Download from site with invalid/self-signed certificate
wget --no-check-certificate https://self-signed.example.com/file.zip

Warning: Only use this for testing or trusted internal sites!

### Example 18: Download with Timestamp Check
# Only download if newer than local file
wget -N https://example.com/updated-file.zip

# Useful for daily/weekly updates:
wget -N https://example.com/daily-report.pdf

### Example 19: Change User Agent
# Some sites block wget, pretend to be a browser:
wget -U "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36" \
     https://example.com/file.zip

# Or use shortcut for common browsers:
wget --user-agent="Mozilla/5.0" https://example.com/file.zip

### Example 20: Download Through Proxy
# Use HTTP proxy
wget -e use_proxy=yes -e http_proxy=proxy.example.com:8080 https://example.com/file.zip

# With authentication:
wget -e use_proxy=yes -e http_proxy=user:pass@proxy.example.com:8080 https://example.com/file.zip

---

## 5. Daily Use Cases

### Use Case 1: Software Installation on Servers (No Internet)

Scenario: You need to install Java on a production server that has no internet access

Step 1: Download on your laptop (has internet)
# On your laptop:
wget https://download.oracle.com/java/17/latest/jdk-17_linux-x64_bin.tar.gz

Step 2: Verify download
ls -lh jdk-17_linux-x64_bin.tar.gz
# Output: -rw-r--r-- 1 user user 181M Nov 21 10:30 jdk-17_linux-x64_bin.tar.gz

Step 3: Transfer to server (via jump server or direct scp)
scp jdk-17_linux-x64_bin.tar.gz username@production-server:/tmp/

Step 4: Install on server
ssh username@production-server
cd /tmp
sudo tar -xzf jdk-17_linux-x64_bin.tar.gz -C /opt/
sudo ln -s /opt/jdk-17 /opt/java

Real-Life Benefits:
✅ No need for direct internet on production server (security!)
✅ Downloaded file can be reused for multiple servers
✅ Downloaded file can be verified/scanned before deployment
✅ Compliance with corporate security policies

### Use Case 2: Automated Daily Backups Download

Scenario: Download daily database backups from remote server to local storage

Create script: backup_downloader.sh

#!/bin/bash
BACKUP_URL="https://backup-server.company.com/daily"
LOCAL_DIR="/backups/database"
DATE=$(date +%Y%m%d)

# Create directory if not exists
mkdir -p $LOCAL_DIR

# Download today's backup with retry
wget -t 5 -T 300 \
     --user=backup_user \
     --password=$(cat /secure/.backup_password) \
     -P $LOCAL_DIR \
     -N \
     "${BACKUP_URL}/db_backup_${DATE}.sql.gz"

# Check if download succeeded
if [ $? -eq 0 ]; then
  echo "Backup downloaded successfully: db_backup_${DATE}.sql.gz"
  # Verify backup integrity
  gunzip -t "${LOCAL_DIR}/db_backup_${DATE}.sql.gz"
  if [ $? -eq 0 ]; then
    echo "Backup integrity verified"
  else
    echo "ERROR: Backup file is corrupted!" | mail -s "Backup Alert" admin@company.com
  fi
else
  echo "ERROR: Backup download failed!" | mail -s "Backup Alert" admin@company.com
fi

# Delete backups older than 30 days
find $LOCAL_DIR -name "db_backup_*.sql.gz" -mtime +30 -delete

Setup cron job:
crontab -e
# Add this line to run daily at 2 AM:
0 2 * * * /scripts/backup_downloader.sh >> /logs/backup_download.log 2>&1

Real-Life Benefits:
✅ Automated disaster recovery preparation
✅ No manual intervention needed
✅ Automatic cleanup of old backups
✅ Email alerts on failures
✅ Integrity verification built-in

### Use Case 3: Website Monitoring and Archiving

Scenario: Archive your company website daily for compliance/legal purposes

Create script: website_archiver.sh

#!/bin/bash
DATE=$(date +%Y%m%d)
ARCHIVE_DIR="/archives/website"
URL="https://www.company.com"

# Create dated directory
mkdir -p ${ARCHIVE_DIR}/${DATE}

# Mirror website
wget --mirror \
     --convert-links \
     --adjust-extension \
     --page-requisites \
     --no-parent \
     --wait=1 \
     --random-wait \
     --limit-rate=500k \
     -P ${ARCHIVE_DIR}/${DATE} \
     ${URL}

# Compress archive
cd ${ARCHIVE_DIR}
tar -czf website_${DATE}.tar.gz ${DATE}/
rm -rf ${DATE}/

# Keep only last 90 days
find ${ARCHIVE_DIR} -name "website_*.tar.gz" -mtime +90 -delete

echo "Website archived: website_${DATE}.tar.gz"

Cron job for monthly archiving:
0 1 1 * * /scripts/website_archiver.sh >> /logs/website_archive.log 2>&1

Real-Life Benefits:
✅ Legal compliance (prove what was published when)
✅ Disaster recovery (full website backup)
✅ Content history tracking
✅ Offline viewing capability
✅ Protection against accidental deletions

### Use Case 4: Batch File Downloads from FTP Server

Scenario: Download daily reports from partner company's FTP server

Create URL list file: ftp_downloads.txt
ftp://reports:password@partner-ftp.example.com/reports/sales_report.csv
ftp://reports:password@partner-ftp.example.com/reports/inventory_report.csv
ftp://reports:password@partner-ftp.example.com/reports/customer_report.csv

Download script: download_reports.sh

#!/bin/bash
REPORTS_DIR="/data/partner_reports"
DATE=$(date +%Y%m%d)

mkdir -p ${REPORTS_DIR}/${DATE}

# Download all reports
wget -i ftp_downloads.txt -P ${REPORTS_DIR}/${DATE} -o ${REPORTS_DIR}/${DATE}/download.log

# Check download status
if [ $? -eq 0 ]; then
  echo "All reports downloaded successfully"
  
  # Process reports (example: import to database)
  for file in ${REPORTS_DIR}/${DATE}/*.csv; do
    echo "Processing: $file"
    # Your processing logic here
    # mysql -u user -p database < import_script.sql
  done
else
  echo "ERROR: Some downloads failed!" | mail -s "Report Download Alert" admin@company.com
fi

Real-Life Benefits:
✅ Automated partner data integration
✅ No manual FTP client needed
✅ Batch processing of multiple files
✅ Scheduled execution via cron
✅ Error handling and notifications

### Use Case 5: Download Software Updates for Offline Installation

Scenario: Monthly patching day - download all updates on one machine, deploy to all servers

Create script: patch_downloader.sh

#!/bin/bash
PATCH_DIR="/patches/$(date +%Y%m)"
mkdir -p ${PATCH_DIR}

# List of patches/updates to download
declare -a PATCHES=(
  "https://security.vendor.com/patches/security-patch-001.rpm"
  "https://security.vendor.com/patches/security-patch-002.rpm"
  "https://updates.vendor.com/updates/app-update-3.5.tar.gz"
  "https://cdn.database.com/updates/db-patch-8.0.35.zip"
)

echo "Downloading patches to ${PATCH_DIR}"

# Download each patch
for patch in "${PATCHES[@]}"; do
  echo "Downloading: $patch"
  wget -P ${PATCH_DIR} -N $patch
  
  if [ $? -eq 0 ]; then
    echo "✓ Downloaded: $(basename $patch)"
  else
    echo "✗ Failed: $(basename $patch)" | tee -a ${PATCH_DIR}/errors.log
  fi
done

# Create checksum file for verification
cd ${PATCH_DIR}
sha256sum * > checksums.sha256

echo "All patches downloaded. Checksum file created."
echo "Transfer to servers using: scp -r ${PATCH_DIR} target-server:/patches/"

Real-Life Benefits:
✅ Single download, multiple deployments
✅ Faster patching process (no repeated downloads)
✅ Bandwidth optimization
✅ Checksum verification for security
✅ Offline server patching capability

### Use Case 6: API Documentation Download

Scenario: Download API documentation for offline reference

# Download Swagger/OpenAPI documentation as HTML
wget --mirror --convert-links --adjust-extension \
     --page-requisites --no-parent \
     https://api.example.com/docs/

# Download specific API doc pages
wget -r -l 2 -k -p -E \
     --accept-regex="(docs|api|reference)" \
     https://api.example.com/documentation/

Real-Life Benefits:
✅ Offline documentation access
✅ Reference during outages
✅ Training materials for new developers
✅ Version-specific documentation archiving
✅ No dependency on vendor website availability

### Use Case 7: Log File Collection from Multiple Servers

Scenario: Collect log files from application servers for analysis

Create script: log_collector.sh

#!/bin/bash
DATE=$(date +%Y%m%d)
LOG_ARCHIVE="/logs/collected/${DATE}"
mkdir -p ${LOG_ARCHIVE}

# List of servers exposing logs via HTTP
declare -a SERVERS=(
  "http://app-server-01.internal/logs/application.log"
  "http://app-server-02.internal/logs/application.log"
  "http://app-server-03.internal/logs/application.log"
)

# Download logs from each server
for server_log in "${SERVERS[@]}"; do
  server_name=$(echo $server_log | cut -d'/' -f3 | cut -d'.' -f1)
  wget -O "${LOG_ARCHIVE}/${server_name}.log" $server_log
done

# Merge all logs for analysis
cat ${LOG_ARCHIVE}/*.log | sort > ${LOG_ARCHIVE}/merged.log

# Analyze for errors
grep -i "error\|exception\|failed" ${LOG_ARCHIVE}/merged.log > ${LOG_ARCHIVE}/errors.log

echo "Logs collected and analyzed: ${LOG_ARCHIVE}"

Real-Life Benefits:
✅ Centralized log collection
✅ Cross-server issue correlation
✅ Automated error detection
✅ Historical log archiving
✅ Simplified troubleshooting

### Use Case 8: Content Delivery Optimization

Scenario: Pre-download files to CDN origin server during low-traffic hours

Create script: cdn_preload.sh

#!/bin/bash
# Download new content releases to CDN origin
CDN_CACHE="/cdn/origin/content"

# List of new release files
wget -i new_releases.txt \
     -P ${CDN_CACHE} \
     --limit-rate=10m \
     --wait=2 \
     -b \
     -o /logs/cdn_preload.log

# After download, trigger CDN cache warming
# curl -X POST https://cdn-api.example.com/cache/warm \
#      -H "Authorization: Bearer TOKEN"

Cron job for night-time preload:
0 2 * * * /scripts/cdn_preload.sh

Real-Life Benefits:
✅ Faster content delivery to users
✅ Reduced origin server load
✅ Bandwidth optimization (scheduled downloads)
✅ Improved user experience
✅ Cost optimization (off-peak downloads)

---

## 6. Real-Life Scenarios

### Scenario 1: Emergency Security Patch Deployment

Problem: Critical security patch released, need to deploy to 50 servers with no internet

Solution using wget:

Step 1: Download patch on bastion host (has internet)
# On bastion host
wget https://security.apache.org/patches/httpd-critical-patch.tar.gz
wget https://security.apache.org/patches/httpd-critical-patch.tar.gz.asc

# Verify signature
gpg --verify httpd-critical-patch.tar.gz.asc httpd-critical-patch.tar.gz

Step 2: Create deployment package
mkdir emergency-patch
cp httpd-critical-patch.tar.gz emergency-patch/
cat > emergency-patch/deploy.sh << 'EOF'
#!/bin/bash
tar -xzf httpd-critical-patch.tar.gz
cd httpd-critical-patch
./configure && make && sudo make install
sudo systemctl restart httpd
EOF
chmod +x emergency-patch/deploy.sh

Step 3: Distribute to all servers
# Create server list
cat > servers.txt << EOF
web-server-01.internal
web-server-02.internal
web-server-03.internal
...
EOF

# Deploy to all
while read server; do
  echo "Deploying to $server"
  scp -r emergency-patch/ $server:/tmp/
  ssh $server "/tmp/emergency-patch/deploy.sh"
done < servers.txt

Result: All 50 servers patched within 30 minutes!

### Scenario 2: Database Migration - Download Large Dump Files

Problem: Migrate 100GB database from old datacenter to new one, unreliable network

Solution:
# Start download with auto-resume
wget -c -t 0 --timeout=30 \
     --limit-rate=50m \
     ftp://old-datacenter:password@ftp.oldsite.com/database/prod_dump.sql.gz

Explanation:
-c: Resume if interrupted
-t 0: Unlimited retries
--timeout=30: 30 second timeout per retry
--limit-rate=50m: Don't saturate network (50 MB/s)

If network dies, just run same command again:
wget -c -t 0 --timeout=30 \
     --limit-rate=50m \
     ftp://old-datacenter:password@ftp.oldsite.com/database/prod_dump.sql.gz

It will resume from where it stopped!

Monitor progress in another terminal:
watch -n 5 'ls -lh prod_dump.sql.gz'

Result: 100GB file downloaded successfully over 4 hours with 3 network interruptions!

### Scenario 3: Compliance Audit - Download Historical Invoices

Problem: Auditor needs all invoices from 2023, stored on vendor portal

Solution:
# Create script to download all invoices
#!/bin/bash
YEAR=2023
BASE_URL="https://vendor-portal.com/invoices"

# Generate URL list for all months
for month in {01..12}; do
  for day in {01..31}; do
    echo "${BASE_URL}/${YEAR}/${month}/${day}/invoice.pdf"
  done
done > invoice_urls.txt

# Download all (skip 404s)
wget -i invoice_urls.txt \
     --user=company123 \
     --password=secretpass \
     -P /audit/invoices_2023/ \
     --no-clobber \
     -nv

# Count successful downloads
find /audit/invoices_2023/ -name "invoice.pdf" | wc -l

Result: Downloaded 365 invoices, ready for audit review!

### Scenario 4: Disaster Recovery - Backup Download Test

Problem: Test disaster recovery by downloading backups from cloud storage

Solution:
#!/bin/bash
# Disaster Recovery Drill
BACKUP_URL="https://backup-cloud.company.com/dr-backups"
RESTORE_DIR="/restore-test"
DATE=$(date +%Y%m%d)

echo "=== DR Drill Started: ${DATE} ==="

# Download full backup
time wget -v \
     --user=dr_admin \
     --password=$(cat /secure/.dr_password) \
     -O ${RESTORE_DIR}/full_backup_${DATE}.tar.gz \
     ${BACKUP_URL}/latest/full_backup.tar.gz

# Verify download
if [ $? -eq 0 ]; then
  echo "✓ Download successful"
  
  # Verify integrity
  echo "Verifying backup integrity..."
  tar -tzf ${RESTORE_DIR}/full_backup_${DATE}.tar.gz > /dev/null
  
  if [ $? -eq 0 ]; then
    echo "✓ Backup integrity verified"
    
    # Test extract
    echo "Testing extraction..."
    mkdir -p ${RESTORE_DIR}/test_extract
    tar -xzf ${RESTORE_DIR}/full_backup_${DATE}.tar.gz -C ${RESTORE_DIR}/test_extract
    
    echo "✓ DR Drill PASSED"
    echo "Download time: Check 'time' output above"
  else
    echo "✗ Backup corrupted!"
    exit 1
  fi
else
  echo "✗ Download failed!"
  exit 1
fi

Cron job for quarterly DR drills:
0 3 1 */3 * /scripts/dr_drill.sh >> /logs/dr_drill.log 2>&1

Result: Quarterly DR drills ensure backup/restore procedures work!

### Scenario 5: Development Environment Setup

Problem: New developer needs to set up local environment, download all dependencies

Create setup script: dev_setup.sh

#!/bin/bash
TOOLS_DIR="/opt/devtools"
mkdir -p ${TOOLS_DIR}

echo "Setting up development environment..."

# Download and install Node.js
wget -P /tmp https://nodejs.org/dist/v18.17.0/node-v18.17.0-linux-x64.tar.xz
tar -xJf /tmp/node-v18.17.0-linux-x64.tar.xz -C ${TOOLS_DIR}

# Download and install VS Code
wget -O /tmp/vscode.deb https://code.visualstudio.com/sha/download?build=stable&os=linux-deb-x64
sudo dpkg -i /tmp/vscode.deb

# Download Docker Compose
sudo wget -O /usr/local/bin/docker-compose \
  https://github.com/docker/compose/releases/download/v2.20.0/docker-compose-linux-x86_64
sudo chmod +x /usr/local/bin/docker-compose

# Download project documentation
wget -r -l 1 -k -p https://docs.company.com/onboarding/

# Download sample databases
wget -P ${TOOLS_DIR}/sample-data \
  https://data.company.com/samples/dev_database.sql.gz

echo "Development environment ready!"

Result: New developer productive within 1 hour instead of full day!

### Scenario 6: IoT Firmware Updates

Problem: Update firmware on 1000 IoT devices, need to download firmware once, distribute locally

Solution:
# Central server downloads firmware
wget -O /firmware/sensor_v2.5.bin \
     https://manufacturer.com/downloads/sensor_firmware_v2.5.bin

# Verify checksum
echo "abc123def456... /firmware/sensor_v2.5.bin" | sha256sum -c

# Distribute to devices over local network
for device in $(cat device_list.txt); do
  echo "Updating device: $device"
  curl -F "firmware=@/firmware/sensor_v2.5.bin" \
       http://${device}/update
done

Result: 1000 devices updated using single firmware download!

### Scenario 7: Static Site Generator with Scheduled Content Updates

Problem: Blog pulls content from CMS, needs to rebuild site when content changes

Script: content_updater.sh

#!/bin/bash
CONTENT_URL="https://cms.example.com/api/content-export.json"
SITE_DIR="/var/www/blog"

# Download latest content
wget -O /tmp/content.json.new ${CONTENT_URL}

# Check if content changed
if ! cmp -s /tmp/content.json.new ${SITE_DIR}/content.json; then
  echo "Content updated, rebuilding site..."
  
  mv /tmp/content.json.new ${SITE_DIR}/content.json
  
  # Rebuild static site
  cd ${SITE_DIR}
  hugo --minify
  
  # Deploy
  rsync -avz public/ webserver:/var/www/html/
  
  echo "Site rebuilt and deployed"
else
  echo "No content changes"
fi

Cron job to check every hour:
0 * * * * /scripts/content_updater.sh >> /logs/content_update.log 2>&1

Result: Automated content publishing system!

---

## 7. Advanced Techniques

### Technique 1: Recursive Download with Filters

# Download only specific file types from entire site
wget -r -l 5 \
     --accept=pdf,doc,docx,xls,xlsx \
     --reject=jpg,png,gif,mp4 \
     http://documents.example.com/

# Download specific directory structure
wget -r -np -nH --cut-dirs=2 \
     http://example.com/path/to/files/

Explanation:
-np: No parent (don't go up)
-nH: No host directories
--cut-dirs=2: Skip first 2 directory levels

### Technique 2: Parallel Downloads Using xargs

# Download multiple URLs in parallel (4 at a time)
cat urls.txt | xargs -n 1 -P 4 wget -q

# More controlled parallel downloads
cat urls.txt | parallel -j 4 "wget {}"

### Technique 3: Bandwidth-Aware Downloading

# Download during off-peak hours at full speed
# Download during business hours with limit

HOUR=$(date +%H)

if [ $HOUR -ge 18 ] || [ $HOUR -le 7 ]; then
  # Off-peak: Full speed
  wget https://example.com/large-file.iso
else
  # Business hours: Limited to 1MB/s
  wget --limit-rate=1m https://example.com/large-file.iso
fi

### Technique 4: Conditional Downloads Based on File Size

# Only download if file is less than 100MB
wget --spider https://example.com/file.zip 2>&1 | grep "Length"

# If size acceptable, download
if [ $(wget --spider https://example.com/file.zip 2>&1 | grep "Length" | awk '{print $2}') -lt 104857600 ]; then
  wget https://example.com/file.zip
else
  echo "File too large, skipping"
fi

### Technique 5: Download with Progress Monitoring

# Download with detailed progress
wget --progress=dot:mega https://example.com/large-file.iso

# Download with custom progress indicator
wget --progress=bar:force:noscroll https://example.com/file.zip

# Monitor download speed in real-time
wget https://example.com/large-file.iso 2>&1 | \
  stdbuf -o0 grep -o "[0-9]*[KMGT]B/s" | \
  while read speed; do
    echo "Current speed: $speed"
  done

### Technique 6: Distributed Downloads Across Multiple Servers

# Split download across multiple mirrors for faster speed
# (Requires manual file merging)

# Mirror 1: Download first 100MB
wget --start-pos=0 --quota=100m -O file.part1 http://mirror1.com/file.iso

# Mirror 2: Download next 100MB  
wget --start-pos=104857600 --quota=100m -O file.part2 http://mirror2.com/file.iso

# Merge files
cat file.part1 file.part2 > file.iso

### Technique 7: Smart Retry with Exponential Backoff

#!/bin/bash
URL="https://unreliable-server.com/file.zip"
MAX_RETRIES=5
WAIT=1

for i in $(seq 1 $MAX_RETRIES); do
  echo "Attempt $i of $MAX_RETRIES"
  
  if wget -t 1 -T 30 $URL; then
    echo "Download successful!"
    exit 0
  fi
  
  if [ $i -lt $MAX_RETRIES ]; then
    echo "Failed, waiting ${WAIT} seconds..."
    sleep $WAIT
    WAIT=$((WAIT * 2))  # Exponential backoff
  fi
done

echo "All retries exhausted, download failed"
exit 1

### Technique 8: Download with Verification

#!/bin/bash
FILE_URL="https://example.com/software.tar.gz"
CHECKSUM_URL="https://example.com/software.tar.gz.sha256"

# Download file
wget $FILE_URL

# Download checksum
wget -q -O - $CHECKSUM_URL | sha256sum -c

if [ $? -eq 0 ]; then
  echo "✓ Checksum verified, file is authentic"
else
  echo "✗ Checksum mismatch, file may be corrupted or tampered!"
  rm software.tar.gz
  exit 1
fi

---

## 8. Best Practices

### Best Practice 1: Always Use Timeouts in Production Scripts
# Bad: Can hang forever
wget https://slow-server.com/file.zip

# Good: Times out after 30 seconds
wget -T 30 https://slow-server.com/file.zip

### Best Practice 2: Implement Proper Error Handling
#!/bin/bash
if wget -q -T 30 https://example.com/file.zip; then
  echo "Download successful"
  # Process file
else
  echo "Download failed" >&2
  # Send alert
  mail -s "Download Failed" admin@company.com < /dev/null
  exit 1
fi

### Best Practice 3: Use Bandwidth Limiting on Production Networks
# Limit to 1MB/s to avoid network congestion
wget --limit-rate=1m https://example.com/large-file.iso

### Best Practice 4: Always Log Downloads
# Create detailed log
wget -o download.log https://example.com/file.zip

# Or append to existing log
wget -a master.log https://example.com/file.zip

### Best Practice 5: Verify Downloads
# Download file and signature
wget https://example.com/file.tar.gz
wget https://example.com/file.tar.gz.asc

# Verify
gpg --verify file.tar.gz.asc file.tar.gz

### Best Practice 6: Use Resume for Large Files
# Always use -c for large files
wget -c https://example.com/large-file.iso

### Best Practice 7: Respect robots.txt in Production
# Default: respects robots.txt
wget -r https://example.com/

# Only disable for internal/authorized scraping
wget -r -e robots=off https://internal.company.com/

### Best Practice 8: Clean Up Temporary Files
#!/bin/bash
TEMP_FILE=$(mktemp)
trap "rm -f $TEMP_FILE" EXIT

wget -O $TEMP_FILE https://example.com/file.zip
# Process file...
# Cleanup happens automatically on exit

### Best Practice 9: Document Download Sources
# Create manifest file
cat > download_manifest.txt << EOF
# Software versions downloaded on $(date)
# Source: https://example.com/downloads/
File: software-v1.2.3.tar.gz
URL: https://example.com/downloads/software-v1.2.3.tar.gz
SHA256: abc123def456...
Downloaded: $(date)
Purpose: Production deployment Q4 2025
EOF

### Best Practice 10: Use Configuration Files for Complex Downloads
# Create wget config file: ~/.wgetrc
timestamping = on
no_parent = on
timeout = 30
tries = 3
wait = 2
user_agent = Mozilla/5.0

# Or project-specific config
wget --config=project-wget.conf -i urls.txt

---

## 9. Common Errors & Solutions

### Error 1: Connection Refused

Error:
wget: unable to resolve host address 'example.com'
Connecting to example.com:80... failed: Connection refused.

Solutions:
# Check DNS resolution
nslookup example.com
dig example.com

# Try with IP address
wget http://192.168.1.100/file.zip

# Check if port is correct
wget http://example.com:8080/file.zip

### Error 2: 403 Forbidden

Error:
HTTP request sent, awaiting response... 403 Forbidden

Solutions:
# Add user agent (some sites block wget)
wget -U "Mozilla/5.0" https://example.com/file.zip

# Add referer
wget --referer="https://example.com" https://example.com/download/file.zip

# Use authentication if required
wget --user=username --password=pass https://example.com/file.zip

### Error 3: 404 Not Found

Error:
HTTP request sent, awaiting response... 404 Not Found

Solutions:
# Verify URL is correct
wget --spider https://example.com/file.zip

# Check with browser first
# Update URL if file moved

### Error 4: SSL Certificate Error

Error:
ERROR: cannot verify example.com's certificate
ERROR: certificate common name 'other.com' doesn't match requested host name 'example.com'

Solutions:
# Check certificate (recommended)
openssl s_client -connect example.com:443

# Temporary bypass (testing only!)
wget --no-check-certificate https://example.com/file.zip

# Proper fix: Update CA certificates
sudo update-ca-certificates
sudo apt-get install ca-certificates

### Error 5: Disk Full

Error:
file.zip: No space left on device

Solutions:
# Check disk space
df -h

# Download to different partition
wget -P /mnt/large-disk/ https://example.com/file.zip

# Clean up old files
find /downloads -mtime +30 -delete

### Error 6: Connection Timeout

Error:
Connecting to example.com... failed: Connection timed out.

Solutions:
# Increase timeout
wget --timeout=60 https://example.com/file.zip

# Increase retries
wget -t 10 https://example.com/file.zip

# Check firewall
sudo iptables -L
telnet example.com 80

### Error 7: Too Many Redirects

Error:
HTTP request sent, awaiting response... 301 Moved Permanently
[...20 redirects later...]
ERROR: 20 redirections exceeded

Solutions:
# Increase redirect limit
wget --max-redirect=30 https://example.com/file.zip

# Follow final location manually
curl -I https://example.com/file.zip  # See final Location
wget <final-url>

### Error 8: Incomplete Download

Error:
Read error at byte 12345678/50000000

Solutions:
# Resume download
wget -c https://example.com/large-file.iso

# If resume doesn't work, remove partial file
rm large-file.iso
wget https://example.com/large-file.iso

### Error 9: Permission Denied

Error:
Cannot write to 'file.zip' (Permission denied).

Solutions:
# Check directory permissions
ls -ld /download/directory/

# Use writable directory
wget -P ~/downloads/ https://example.com/file.zip

# Fix permissions
sudo chown user:user /download/directory/
chmod 755 /download/directory/

### Error 10: Quota Exceeded

Error:
Download quota of 1000M bytes exceeded!

Solutions:
# Remove quota
wget --quota=inf https://example.com/file.zip

# Increase quota
wget --quota=5000m https://example.com/file.zip

---

## 10. Quick Reference

### Most Common Commands

# Simple download
wget https://example.com/file.zip

# Download with custom name
wget -O myfile.zip https://example.com/file.zip

# Resume interrupted download
wget -c https://example.com/large-file.iso

# Background download
wget -b https://example.com/file.zip

# Download from file list
wget -i urls.txt

# Mirror website
wget -m https://example.com

# Download with authentication
wget --user=admin --password=secret https://example.com/file.zip

# Limit download speed
wget --limit-rate=500k https://example.com/file.zip

# Quiet mode (no output)
wget -q https://example.com/file.zip

# Check if URL exists
wget --spider https://example.com/file.zip

### wget vs curl Quick Comparison

Task: Download file
wget: wget https://example.com/file.zip
curl: curl -O https://example.com/file.zip

Task: Resume download
wget: wget -c https://example.com/file.zip
curl: curl -C - -O https://example.com/file.zip

Task: Save with custom name
wget: wget -O myfile.zip https://example.com/file.zip
curl: curl -o myfile.zip https://example.com/file.zip

Task: Background download
wget: wget -b https://example.com/file.zip
curl: curl -O https://example.com/file.zip &

Task: Download multiple files
wget: wget -i urls.txt
curl: (requires loop or xargs)

Task: Mirror website
wget: wget -m https://example.com
curl: Not supported

Task: API testing
wget: Limited support
curl: curl -X POST -d '{"key":"value"}' https://api.example.com

---

## 11. Summary

### Key Points to Remember:

1. **wget is for downloading** - Best for file downloads, especially large files
2. **curl is for data transfer** - Best for API testing and complex HTTP operations
3. **Resume capability** - wget excels at handling interrupted downloads
4. **Corporate environments** - wget perfect for servers without internet access
5. **Automation** - Excellent for scripts, cron jobs, and automated workflows
6. **Recursive downloads** - Can mirror entire websites
7. **Background operation** - Native support for background downloads

### When to Choose wget:
✅ Downloading large files (ISOs, backups, databases)
✅ Mirroring websites for offline viewing
✅ Batch downloading multiple files
✅ Unreliable network connections (auto-resume)
✅ Corporate servers without direct internet
✅ Automated backup downloads
✅ Long-running downloads

### When to Choose curl:
✅ API development and testing
✅ Testing webhooks
✅ Viewing HTTP headers
✅ Uploading files
✅ Complex authentication scenarios
✅ Quick HTTP status checks
✅ Piping data to other commands

### Corporate Workflow Reminder:
┌────────────┐     wget      ┌───────────┐     scp      ┌──────────────┐
│ Your Laptop├──────────────▶│ Jump Host ├─────────────▶│ Prod Server  │
│ (Internet) │   download    │ (Gateway) │   transfer   │(No Internet) │
└────────────┘               └───────────┘              └──────────────┘

---

## Author Information
Guide based on image by: Imran Afzal
Website: www.utclisolutions.com

For more Linux tutorials and DevOps guides, visit the website!
